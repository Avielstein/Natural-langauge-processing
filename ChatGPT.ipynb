{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b75c418-69ff-4ec9-bbab-619c3037e717",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Asynchronous OpenAI API Interaction Tutorial\n",
    "\n",
    "### Project Overview\n",
    "This section of our project focuses on demonstrating the power and flexibility of asynchronous programming when interacting with the OpenAI API, specifically leveraging the GPT-3.5-turbo model. It aims to provide a comprehensive guide and practical examples for developers looking to integrate OpenAI's cutting-edge AI capabilities into their applications efficiently, without blocking their application's runtime operations.\n",
    "\n",
    "### Key Features\n",
    "- Asynchronous API Calls: Showcases how to make non-blocking calls to the OpenAI API, allowing for faster and more efficient application performance.\n",
    "- Practical Use Cases: Includes examples for generating text completions, summarizing text, answering questions, and processing multiple requests concurrently, all in an asynchronous manner.\n",
    "- Configurable and Secure: Demonstrates best practices for securely loading API keys from configuration files, ensuring that your application remains secure and configurable.\n",
    "- Environment Compatibility: Tailored to work seamlessly in Jupyter Lab and similar environments, facilitating easy integration and experimentation with OpenAI's API in a notebook format.\n",
    "\n",
    "### Getting Started\n",
    "The tutorial assumes familiarity with Python asynchronous programming concepts (asyncio) and requires the OpenAI Python package. It guides you through setting up your environment, installing necessary packages, and configuring your OpenAI API key securely.\n",
    "\n",
    "### Examples and Usage\n",
    "Detailed examples walk you through the process of making asynchronous requests to the OpenAI API for various tasks. Each example is designed to provide a foundation upon which you can build more complex interactions tailored to your specific needs.\n",
    "\n",
    "For developers looking to integrate AI functionalities into their projects without compromising on performance, this project section offers valuable insights and tools. Whether you're generating dynamic content, building conversational agents, or processing large volumes of requests, these examples serve as a practical starting point.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db58c869-3f0e-45d0-862f-d8d3a9475fca",
   "metadata": {},
   "source": [
    "### Set up comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e49eb73d-0489-47b3-8624-bdddcd1f40bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install openai\n",
    "#!npm install openai@^4.0.0\n",
    "#!node -v\n",
    "#!echo $PATH\n",
    "\n",
    "#was having issues with the path for node\n",
    "#import os\n",
    "#os.environ['PATH'] += ':/usr/local/bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58550e04-2f0a-4eb8-ad34-73afad29021f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "version: 1.13.3\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import openai  # Ensure this import is here\n",
    "# new\n",
    "from openai import OpenAI\n",
    "from openai import AsyncOpenAI\n",
    "#!pip install --upgrade openai ## another way to update\n",
    "#!openai migrate #use this to update the version\n",
    "print('version:',openai.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4d6016-0535-425e-a220-d8b1fb60167f",
   "metadata": {},
   "source": [
    "### Get Key From Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5cd74574-b2f2-4bdf-a4ec-e5b044e3cbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load API key from 'config.json'\n",
    "with open('config.json', 'r') as f:\n",
    "    config = json.load(f)\n",
    "    api_key = config['OPENAI_API_KEY']\n",
    "\n",
    "# Initialize the OpenAI client with the API key\n",
    "client = AsyncOpenAI(api_key=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87f1270-014e-471e-ac56-678c0a376207",
   "metadata": {},
   "source": [
    "Your config should look like this:```{'OPENAI_API_KEY': '<YOURKEYHERE>'}```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed4f9be-d3ee-4106-983b-9a11362f5909",
   "metadata": {},
   "source": [
    "### TESTING EXAMPLES\n",
    "\n",
    "Run in Asynchronous Notebook Cells\n",
    "If you're using an environment like Jupyter notebook that supports %run magic for asynchronous code, you can directly execute asynchronous code in a cell without wrapping it in asyncio.run(). Just define your async function and then call it using await:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b971abf-6f7b-44b7-813d-0fbea95ae424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-8y2wdGDlLRz3GGzLjd8iEs1pQVGmc', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Hello! How can I assist you today?', role='assistant', function_call=None, tool_calls=None))], created=1709321691, model='gpt-3.5-turbo-0125', object='chat.completion', system_fingerprint='fp_2b778c6b35', usage=CompletionUsage(completion_tokens=9, prompt_tokens=9, total_tokens=18))\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "\n",
    "async def fetch_chat_completion():\n",
    "    completion = await client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[{\"role\": \"user\", \"content\": \"Hello world\"}]\n",
    "    )\n",
    "    return completion\n",
    "\n",
    "# Directly await the coroutine in the notebook cell\n",
    "# Ensure your notebook environment supports this syntax\n",
    "completion = await fetch_chat_completion()\n",
    "print(completion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e921ec0e-3c7f-49ef-97b6-6600e62b5bac",
   "metadata": {},
   "source": [
    "#### Example 1: Generating Text Completions\n",
    "This example shows how to generate a more complex text completion, simulating a conversation with the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddcd5a38-cf56-418b-98d2-5db32a91e220",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'client' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m completion\n\u001b[1;32m     11\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTell me a joke about AI.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 12\u001b[0m completion \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m generate_text_completion(prompt)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Adjusted access to the message content\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(completion\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent)\n",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m, in \u001b[0;36mgenerate_text_completion\u001b[0;34m(prompt)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_text_completion\u001b[39m(prompt):\n\u001b[0;32m----> 2\u001b[0m     completion \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m client\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m      3\u001b[0m         model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-3.5-turbo\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      4\u001b[0m         messages\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m      5\u001b[0m             {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are a witty assistant.\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m      6\u001b[0m             {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt}\n\u001b[1;32m      7\u001b[0m         ]\n\u001b[1;32m      8\u001b[0m     )\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m completion\n",
      "\u001b[0;31mNameError\u001b[0m: name 'client' is not defined"
     ]
    }
   ],
   "source": [
    "async def generate_text_completion(prompt):\n",
    "    completion = await client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a witty assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "    return completion\n",
    "\n",
    "prompt = \"Tell me a joke about AI.\"\n",
    "completion = await generate_text_completion(prompt)\n",
    "\n",
    "# Adjusted access to the message content\n",
    "print(completion.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84694899-0372-4b88-93ba-c670bffa1ace",
   "metadata": {},
   "source": [
    "#### Example 2: Summarizing Text\n",
    "This example demonstrates how to use the model to summarize a longer piece of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ea13b0c-b43e-4b56-a5ba-2c895ac98e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text explains that artificial intelligence (AI) is the simulation of human intelligence in machines, allowing them to think like humans. This may involve programming machines to learn and solve problems, exhibiting traits associated with the human mind.\n"
     ]
    }
   ],
   "source": [
    "async def summarize_text(text):\n",
    "    completion = await client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Please summarize the following text.\"},\n",
    "            {\"role\": \"user\", \"content\": text}\n",
    "        ]\n",
    "    )\n",
    "    return completion\n",
    "\n",
    "text = \"\"\"\n",
    "Artificial intelligence (AI) refers to the simulation of human intelligence in machines that are programmed to think like humans. The term may also be applied to any machine that exhibits traits associated with a human mind such as learning and problem-solving.\n",
    "\"\"\"\n",
    "summary = await summarize_text(text)\n",
    "print(summary.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc24f22c-ee03-41f8-a8c8-31da42a0ec6e",
   "metadata": {},
   "source": [
    "#### Example 3: Answering Questions\n",
    "This example sets up the model to answer specific questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d784edb7-e1f1-4539-b870-de04108b7329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average distance from Earth to the Moon is about 384,400 kilometers (238,855 miles).\n"
     ]
    }
   ],
   "source": [
    "async def ask_question(question):\n",
    "    completion = await client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an assistant capable of answering questions.\"},\n",
    "            {\"role\": \"user\", \"content\": question}\n",
    "        ]\n",
    "    )\n",
    "    return completion\n",
    "\n",
    "question = \"What is the distance from Earth to the Moon?\"\n",
    "answer = await ask_question(question)\n",
    "print(answer.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0f7a10-4bce-4984-9799-3b56159c742a",
   "metadata": {},
   "source": [
    "#### Example 4: Running Multiple Asynchronous Requests\n",
    "Demonstrating how to process multiple asynchronous requests concurrently, useful for batching tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0244c4e2-919f-4159-838d-acd5797b92ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The future of AI is exciting and full of possibilities! With advancements in machine learning, automation, and natural language processing, AI will likely continue to revolutionize various industries such as healthcare, finance, and entertainment. However, the ethical and societal implications of AI will also need to be carefully considered to ensure a positive impact on society. In short, the future of AI holds both great promise and important challenges to address.\n",
      "Sure! Quantum computing is a type of computing that uses the principles of quantum mechanics to perform operations on data. In traditional computing, data is stored in bits that can be either a 0 or a 1. However, in quantum computing, data is stored in quantum bits, or qubits, which can exist as both a 0 and a 1 at the same time, thanks to a phenomenon called superposition. This allows quantum computers to perform complex calculations much faster than classical computers for certain tasks. It's like having a computer that can explore multiple solutions to a problem simultaneously, which can be really powerful for certain types of problems.\n"
     ]
    }
   ],
   "source": [
    "async def batch_process_prompts(prompts):\n",
    "    tasks = [generate_text_completion(prompt) for prompt in prompts]\n",
    "    completions = await asyncio.gather(*tasks)\n",
    "    return completions\n",
    "\n",
    "prompts = [\"What's the future of AI?\", \"Explain quantum computing in simple terms.\"]\n",
    "completions = await batch_process_prompts(prompts)\n",
    "\n",
    "for completion in completions:\n",
    "    print(completion.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d890f2c-a1e7-4241-b778-611ba3d8404b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
