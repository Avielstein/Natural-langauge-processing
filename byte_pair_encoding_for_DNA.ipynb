{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Byte Pair Encoding and DNA Sequence Classification with Deep Learning\n",
    "\n",
    "This project utilizes Byte Pair Encoding (BPE) for tokenization and applies deep learning techniques to classify DNA sequences. It draws inspiration and knowledge from various sources, including Kaggle and scientific literature, to implement a solution that leverages SentencePiece for BPE and PyTorch for constructing a neural network model.\n",
    "\n",
    "## Dependencies\n",
    "\n",
    "- SentencePiece: For tokenization using Byte Pair Encoding.\n",
    "- NumPy: For linear algebra operations.\n",
    "- Pandas: For data processing and CSV file I/O.\n",
    "- Scikit-learn: For feature extraction through CountVectorizer.\n",
    "- PyTorch: For creating and training the neural network model.\n",
    "\n",
    "## Dataset\n",
    "\n",
    "The dataset comprises DNA sequences from humans, chimpanzees, and dogs. The aim is to classify these sequences into various categories based on their genetic information.\n",
    "\n",
    "## Preprocessing\n",
    "\n",
    "The preprocessing steps involve converting DNA sequences into k-mers, which are subsequences of length k. This representation is then used to transform the sequences into numerical format suitable for machine learning models.\n",
    "\n",
    "## Execution Steps\n",
    "\n",
    "1. Preprocess the data to generate k-mers.\n",
    "2. Train the SentencePiece model for tokenization.\n",
    "3. Load the tokenized data into a PyTorch DataLoader.\n",
    "4. Define the neural network model, loss function, and optimizer.\n",
    "5. Train the model using the training dataset.\n",
    "6. Validate the model's performance on a test dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/chuckzzzz/dna-classification-with-deep-learning-part-1/notebook\n",
    "#https://www.kaggle.com/nageshsingh/demystify-dna-sequencing-with-machine-learning/notebook\n",
    "#https://pypi.org/project/sentencepiece/\n",
    "#http://ethen8181.github.io/machine-learning/deep_learning/subword/bpe.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "Utilizes SentencePiece for Byte Pair Encoding, which is crucial for handling the vast vocabulary inherent in DNA sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "model_name = 'genes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "\n",
    "    def __init__(self, filepath=model_name+'.model'):\n",
    "        self.sp = spm.SentencePieceProcessor(model_file=filepath)\n",
    "\n",
    "    def encode(self, text, t=int):\n",
    "        return self.sp.encode(text, out_type=t)\n",
    "\n",
    "    def decode(self, pieces):\n",
    "        return self.sp.decode(pieces)\n",
    "\n",
    "    @staticmethod\n",
    "    def train(input_file='data/raw_sents.txt', model_prefix='sp_model', vocab_size=30522):\n",
    "        spm.SentencePieceTrainer.train(input=input_file, model_prefix=model_prefix, vocab_size=vocab_size,\n",
    "                                       #input_sentence_size=2 ** 16, shuffle_input_sentence=True)\n",
    "                                       input_sentence_size=number_of_lines, shuffle_input_sentence=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train\n",
    "Tokenizer.train(input_file='data.txt', model_prefix='python_tokenizer_30k', vocab_size=30000) #model_prefix is model storage name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiate tokenizer model\n",
    "tokenizer = Tokenizer('python_tokenizer.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize code (ie encode)\n",
    "#with words\n",
    "#tokens = tokenizer.encode(data[1][0],t=str)\n",
    "#with numbers\n",
    "tokens = tokenizer.encode(data[1][0])\n",
    "decoded = tokenizer.decode(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import Counter\n",
    "import random\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Background\n",
    "The basic background of the data used here is exceptionally covered in the kernel provided by the uploader for this dataset [1]. In this notebook, we will base the analysis on kmer-counting and expand on this strategy using various models.\n",
    "\n",
    "https://en.wikipedia.org/wiki/K-mer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. PreprocessÂ¶\n",
    "Most of the preprocessing done here is based on [1]. The main difference is how kmer is represented as a matrix. The original method used a 4-gramers for 6-mers. This should be equivalent to a 9-mer theoratically. For instance, [\"ACCAT\",\"CCACTA\",\"CACTAA\",\"ACTAAG\"] is equivalent to \"ACCACTAG\" in terms of the information it contains. Therefore, the preprocessing here does kmer counting for all the train sequences. It then selects the top X number of most frequent kmer as feature to transform the each sequence. For example, if a sequence is \"ACGTAGACGT\" and the top most frequented kmer feature is [\"ACGT\",\"GTAG\",\"GGCC\"], the transfromed matrix for this sequence is [2,1,0].\n",
    "\n",
    "Since human, chimpanzee and dog are all mammals, we will consider these DNA data as a single dataset. The train and validation split is done on this combined dataset as well. 80% of the sequences are used for training and the rest is used for test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getKmers(sequence, size=8):\n",
    "    return [sequence[x:x+size].lower() for x in range(len(sequence) - size + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "human = pd.read_table('human.txt')\n",
    "chimp = pd.read_table('chimpanzee.txt')\n",
    "dog = pd.read_table('dog.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6    1702\n",
      "4     910\n",
      "3     772\n",
      "0     753\n",
      "1     551\n",
      "2     476\n",
      "5     342\n",
      "Name: class, dtype: int64\n",
      "6    422\n",
      "1    243\n",
      "3    223\n",
      "4    197\n",
      "0    143\n",
      "2     81\n",
      "5     67\n",
      "Name: class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def generate_dataset(dfs,kmer_size,max_features,split=5):\n",
    "    kmer_dfs=[]\n",
    "    for cur_df in dfs:\n",
    "        cur_df['words']=cur_df.apply(lambda x: getKmers(x['sequence'],size=kmer_size), axis=1)\n",
    "        cur_df=cur_df.drop('sequence',axis=1)\n",
    "        kmer_dfs.append(cur_df)\n",
    "    all_data=pd.concat(kmer_dfs).reset_index(drop=True)\n",
    "    perm=np.random.permutation(len(all_data)) #shuffle the data\n",
    "    test_data=all_data[:len(all_data)//split]\n",
    "    train_data=all_data[len(all_data)//split:]\n",
    "    train_kmers=[]\n",
    "    for cur_kmer_list in train_data.words.values:\n",
    "        train_kmers.extend(cur_kmer_list)\n",
    "    vectorizer = CountVectorizer(max_features=max_features).fit(train_kmers) \n",
    "    \n",
    "    print(train_data[\"class\"].value_counts())\n",
    "    print(test_data[\"class\"].value_counts())\n",
    "    \n",
    "    X_train=[]\n",
    "    Y_train=[]\n",
    "    X_test=[]\n",
    "    Y_test=[]\n",
    "    for cur_data, label in zip(train_data['words'],train_data['class']):\n",
    "        cur_transformed=vectorizer.transform(cur_data)\n",
    "        X_train.append(cur_transformed.toarray().sum(axis=0))\n",
    "        Y_train.append(label)\n",
    "    for cur_data, label in zip(test_data['words'],test_data['class']):\n",
    "        cur_transformed=vectorizer.transform(cur_data)\n",
    "        X_test.append(cur_transformed.toarray().sum(axis=0))\n",
    "        Y_test.append(label)  \n",
    "    return X_train, Y_train, X_test, Y_test\n",
    "X_train, Y_train, X_test, Y_test=generate_dataset([human,chimp,dog],kmer_size=9,max_features=1500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model\n",
    "For this kernel, we will leverage the power of deep learning using Pytorch. The most simple model is a feedforward neural network composed of an input layer, several hidden layers and a final output layer. The number of nodes in input layer represents the number of features and the number of nodes in output layer represents number of classes. Here is a general schematic of what a feedforward neural network looks like:\n",
    "\n",
    "FNN Schematic\n",
    "\n",
    "The specific configuration of our experimental model has two hidden layers. We can fine tune model structure after we have shown the prototye works fine on our data (i.e. the model is learning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as data_utils\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(nn.Module):\n",
    "    def __init__(self, in_dim, n_hidden_1, n_hidden_2, out_dim):\n",
    "        super(NN, self).__init__()\n",
    "        self.in_dim=in_dim\n",
    "        self.n_hidden_1=n_hidden_1\n",
    "        self.n_hidden_2=n_hidden_2\n",
    "        self.out_dim=out_dim\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Linear(in_dim, n_hidden_1),\n",
    "            nn.ReLU(True))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Linear(n_hidden_1, n_hidden_2),\n",
    "            nn.ReLU(True))\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Linear(n_hidden_2, out_dim),\n",
    "            nn.ReLU(True))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Numpy to Tensor\n",
    "This might be an over-simplification, but deep learning at its core is really just a lot of matrix operations. These matrices are termed tensor. Despite its different nomenclature, it does support most of the functions we can find in an numpy array. If interested, you may google the difference between numpy arrays and tensors. But basically, tensor is a multi-dimensional array that can operate on both cpu and gpu. When on GPU, the matrix operation will be significantly faster than numpy array on CPU as shwon in the diagram below thanks to this benchmark[3].\n",
    "\n",
    "diagram of runtimeThis is why many deep learning projects use GPU to do training instead of CPU. But since this DNA sequence dataset isn't that much and our model is fairly simple, CPU works just fine. We will use some built-in functions of Pytorch to convert numpy arrays into tensors and dataloaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data_utils.TensorDataset(torch.tensor(X_train), torch.tensor(Y_train))\n",
    "train_loader = data_utils.DataLoader(train_data, batch_size=4, shuffle=True)\n",
    "test_data=data_utils.TensorDataset(torch.tensor(X_test), torch.tensor(Y_test))\n",
    "test_loader=data_utils.DataLoader(test_data, batch_size=4, shuffle=True)\n",
    "n_features=len(train_data[0][0])\n",
    "n_labels=7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 HyperparametersÂ¶\n",
    "Deep learning also involves controlling parameters that are not part of the model itself. These parameters are termed hyperparameters as they control how the model learns from training data. For experimentation, we used emprically reasonable values for these parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=5e-3\n",
    "num_epochs=10\n",
    "torch.manual_seed(2021)\n",
    "model = NN(n_features, n_features//5, n_features//10, n_labels)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,num_epochs,train_loader,criterion,optimizer,verbose=False,learning_rate=5e-3):\n",
    "    for epoch in range(num_epochs):\n",
    "        if(verbose):\n",
    "            print('*' * 10)\n",
    "            print(f'epoch {epoch+1}')\n",
    "        running_loss = 0.0\n",
    "        running_acc = 0.0\n",
    "        for i, data in enumerate(train_loader, 1):\n",
    "            cur_seq, label = data\n",
    "            cur_seq=cur_seq.float()\n",
    "            label=label.squeeze_()\n",
    "            optimizer.zero_grad()\n",
    "            out = model(cur_seq)\n",
    "            loss = criterion(out, label)\n",
    "            running_loss += loss.item()\n",
    "            _, pred = torch.max(out, 1)\n",
    "            running_acc += (pred == label).float().mean()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if (verbose):\n",
    "                if i % 400 == 0:\n",
    "                    print(f'[{epoch+1}/{num_epochs}] Loss: {running_loss/i:.6f}, Acc: {running_acc/i:.6f}')\n",
    "        if(verbose):\n",
    "            print(f'Finish {epoch+1} epoch, Loss: {running_loss/i:.6f}, Acc: {running_acc/i:.6f}')\n",
    "    return running_acc/i, model\n",
    "def validate_model(model,test_loader):\n",
    "        # eval\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            cur_seq, labels = data\n",
    "            cur_seq=cur_seq.float()\n",
    "            labels=labels.squeeze_()\n",
    "            # calculate outputs by running images through the network\n",
    "            out = model(cur_seq)\n",
    "            _, predicted = torch.max(out, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return correct/total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3 Model Sanity TestÂ¶\n",
    "It's usally good practice to try overfitting the model on the traning data first. If the model can overfit the data, that means this dataset is learnable by our naive implementation of neural network. Clearly, overfitting will hurt the model performance on unseen data as shown by the diagram below. However, avoiding overfitting would be a later job once we confirm the model is learning from the dataset. Red is the error on test dataset while blue is the error on training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training accuracy is 0.9898\n"
     ]
    }
   ],
   "source": [
    "acc,model=train_model(model,20,train_loader,criterion,optimizer,verbose=False,learning_rate=5e-3)\n",
    "print(\"Model training accuracy is %.4f\"%acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8023255813953488"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate_model(model,test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=5e-3\n",
    "num_epochs=25\n",
    "torch.manual_seed(2021)\n",
    "model = NN(n_features, n_features//5, n_features//10, n_labels)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "epoch 1\n",
      "[1/25] Loss: 1.899268, Acc: 0.306250\n",
      "[1/25] Loss: 1.875588, Acc: 0.301250\n",
      "[1/25] Loss: 1.852687, Acc: 0.305208\n",
      "Finish 1 epoch, Loss: 1.844667, Acc: 0.306463\n",
      "**********\n",
      "epoch 2\n",
      "[2/25] Loss: 1.759007, Acc: 0.308125\n",
      "[2/25] Loss: 1.742570, Acc: 0.311875\n",
      "[2/25] Loss: 1.712514, Acc: 0.328542\n",
      "Finish 2 epoch, Loss: 1.701493, Acc: 0.332062\n",
      "**********\n",
      "epoch 3\n",
      "[3/25] Loss: 1.579501, Acc: 0.411875\n",
      "[3/25] Loss: 1.522568, Acc: 0.434687\n",
      "[3/25] Loss: 1.492915, Acc: 0.455208\n",
      "Finish 3 epoch, Loss: 1.476195, Acc: 0.468773\n",
      "**********\n",
      "epoch 4\n",
      "[4/25] Loss: 1.288463, Acc: 0.588125\n",
      "[4/25] Loss: 1.254246, Acc: 0.608125\n",
      "[4/25] Loss: 1.205462, Acc: 0.626250\n",
      "Finish 4 epoch, Loss: 1.187723, Acc: 0.633624\n",
      "**********\n",
      "epoch 5\n",
      "[5/25] Loss: 0.992845, Acc: 0.700000\n",
      "[5/25] Loss: 0.981655, Acc: 0.709687\n",
      "[5/25] Loss: 0.933177, Acc: 0.718958\n",
      "Finish 5 epoch, Loss: 0.914140, Acc: 0.725853\n",
      "**********\n",
      "epoch 6\n",
      "[6/25] Loss: 0.705540, Acc: 0.785000\n",
      "[6/25] Loss: 0.668124, Acc: 0.806875\n",
      "[6/25] Loss: 0.664383, Acc: 0.808125\n",
      "Finish 6 epoch, Loss: 0.653697, Acc: 0.813181\n",
      "**********\n",
      "epoch 7\n",
      "[7/25] Loss: 0.530431, Acc: 0.858125\n",
      "[7/25] Loss: 0.509946, Acc: 0.866250\n",
      "[7/25] Loss: 0.504055, Acc: 0.866458\n",
      "Finish 7 epoch, Loss: 0.492592, Acc: 0.868373\n",
      "**********\n",
      "epoch 8\n",
      "[8/25] Loss: 0.388274, Acc: 0.895625\n",
      "[8/25] Loss: 0.388934, Acc: 0.898125\n",
      "[8/25] Loss: 0.382513, Acc: 0.901875\n",
      "Finish 8 epoch, Loss: 0.379888, Acc: 0.903050\n",
      "**********\n",
      "epoch 9\n",
      "[9/25] Loss: 0.322421, Acc: 0.921875\n",
      "[9/25] Loss: 0.315076, Acc: 0.921875\n",
      "[9/25] Loss: 0.304592, Acc: 0.923750\n",
      "Finish 9 epoch, Loss: 0.301720, Acc: 0.925018\n",
      "**********\n",
      "epoch 10\n",
      "[10/25] Loss: 0.251318, Acc: 0.931875\n",
      "[10/25] Loss: 0.254367, Acc: 0.936875\n",
      "[10/25] Loss: 0.245316, Acc: 0.941875\n",
      "Finish 10 epoch, Loss: 0.246025, Acc: 0.941358\n",
      "**********\n",
      "epoch 11\n",
      "[11/25] Loss: 0.210324, Acc: 0.951250\n",
      "[11/25] Loss: 0.209190, Acc: 0.950938\n",
      "[11/25] Loss: 0.204338, Acc: 0.952083\n",
      "Finish 11 epoch, Loss: 0.200322, Acc: 0.953159\n",
      "**********\n",
      "epoch 12\n",
      "[12/25] Loss: 0.174860, Acc: 0.958750\n",
      "[12/25] Loss: 0.169709, Acc: 0.962188\n",
      "[12/25] Loss: 0.169054, Acc: 0.961042\n",
      "Finish 12 epoch, Loss: 0.164325, Acc: 0.962600\n",
      "**********\n",
      "epoch 13\n",
      "[13/25] Loss: 0.135613, Acc: 0.968125\n",
      "[13/25] Loss: 0.136570, Acc: 0.969375\n",
      "[13/25] Loss: 0.140616, Acc: 0.967083\n",
      "Finish 13 epoch, Loss: 0.140205, Acc: 0.966957\n",
      "**********\n",
      "epoch 14\n",
      "[14/25] Loss: 0.113776, Acc: 0.976250\n",
      "[14/25] Loss: 0.115613, Acc: 0.978125\n",
      "[14/25] Loss: 0.119620, Acc: 0.977708\n",
      "Finish 14 epoch, Loss: 0.119367, Acc: 0.977124\n",
      "**********\n",
      "epoch 15\n",
      "[15/25] Loss: 0.092612, Acc: 0.985625\n",
      "[15/25] Loss: 0.103170, Acc: 0.980625\n",
      "[15/25] Loss: 0.103480, Acc: 0.979583\n",
      "Finish 15 epoch, Loss: 0.103697, Acc: 0.979666\n",
      "**********\n",
      "epoch 16\n",
      "[16/25] Loss: 0.085020, Acc: 0.982500\n",
      "[16/25] Loss: 0.088942, Acc: 0.982500\n",
      "[16/25] Loss: 0.086858, Acc: 0.983125\n",
      "Finish 16 epoch, Loss: 0.091047, Acc: 0.981663\n",
      "**********\n",
      "epoch 17\n",
      "[17/25] Loss: 0.086170, Acc: 0.983750\n",
      "[17/25] Loss: 0.084507, Acc: 0.983125\n",
      "[17/25] Loss: 0.082602, Acc: 0.984583\n",
      "Finish 17 epoch, Loss: 0.081167, Acc: 0.984568\n",
      "**********\n",
      "epoch 18\n",
      "[18/25] Loss: 0.063812, Acc: 0.988125\n",
      "[18/25] Loss: 0.071134, Acc: 0.985625\n",
      "[18/25] Loss: 0.072211, Acc: 0.986458\n",
      "Finish 18 epoch, Loss: 0.072344, Acc: 0.987110\n",
      "**********\n",
      "epoch 19\n",
      "[19/25] Loss: 0.063299, Acc: 0.987500\n",
      "[19/25] Loss: 0.064956, Acc: 0.987500\n",
      "[19/25] Loss: 0.066575, Acc: 0.987708\n",
      "Finish 19 epoch, Loss: 0.066268, Acc: 0.987654\n",
      "**********\n",
      "epoch 20\n",
      "[20/25] Loss: 0.068004, Acc: 0.989375\n",
      "[20/25] Loss: 0.058766, Acc: 0.990000\n",
      "[20/25] Loss: 0.060399, Acc: 0.989792\n",
      "Finish 20 epoch, Loss: 0.059200, Acc: 0.989833\n",
      "**********\n",
      "epoch 21\n",
      "[21/25] Loss: 0.059488, Acc: 0.988750\n",
      "[21/25] Loss: 0.054946, Acc: 0.990000\n",
      "[21/25] Loss: 0.057061, Acc: 0.989375\n",
      "Finish 21 epoch, Loss: 0.055834, Acc: 0.990015\n",
      "**********\n",
      "epoch 22\n",
      "[22/25] Loss: 0.050835, Acc: 0.990625\n",
      "[22/25] Loss: 0.053769, Acc: 0.990313\n",
      "[22/25] Loss: 0.054853, Acc: 0.989583\n",
      "Finish 22 epoch, Loss: 0.051757, Acc: 0.990741\n",
      "**********\n",
      "epoch 23\n",
      "[23/25] Loss: 0.046719, Acc: 0.992500\n",
      "[23/25] Loss: 0.049417, Acc: 0.991562\n",
      "[23/25] Loss: 0.047374, Acc: 0.991250\n",
      "Finish 23 epoch, Loss: 0.048397, Acc: 0.991104\n",
      "**********\n",
      "epoch 24\n",
      "[24/25] Loss: 0.051556, Acc: 0.988750\n",
      "[24/25] Loss: 0.049336, Acc: 0.990625\n",
      "[24/25] Loss: 0.047240, Acc: 0.991250\n",
      "Finish 24 epoch, Loss: 0.047156, Acc: 0.991467\n",
      "**********\n",
      "epoch 25\n",
      "[25/25] Loss: 0.045782, Acc: 0.991250\n",
      "[25/25] Loss: 0.041545, Acc: 0.993438\n",
      "[25/25] Loss: 0.042698, Acc: 0.992917\n",
      "Finish 25 epoch, Loss: 0.042743, Acc: 0.992375\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8088662790697675"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc,model=train_model(model,num_epochs,train_loader,criterion,optimizer,verbose=True,learning_rate=5e-3)\n",
    "validate_model(model,test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "https://www.kaggle.com/nageshsingh/demystify-dna-sequencing-with-machine-learning#Demystify-DNA-Sequencing-with-Machine-Learning\n",
    "\n",
    "\n",
    "https://upload.wikimedia.org/wikipedia/commons/c/c2/MultiLayerNeuralNetworkBigger_english.png\n",
    "\n",
    "\n",
    "https://medium.com/thenoobengineer/numpy-arrays-vs-tensors-c58ea54f0e59\n",
    "\n",
    "\n",
    "https://upload.wikimedia.org/wikipedia/commons/f/fc/Overfitting.png\n",
    "\n",
    "\n",
    "https://github.com/L1aoXingyu/pytorch-beginner/tree/master/03-Neural%20Network"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
